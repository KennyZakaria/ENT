import faiss
import numpy as np
import pickle
import os
import re
from typing import List, Dict, Tuple, Optional
from sentence_transformers import SentenceTransformer, CrossEncoder
from utils.config import config
from services.document_service import document_service

class RAGService:
    def __init__(self):
        # Mod√®le d'embedding pour la recherche initiale
        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')
        
        # Mod√®le de reranking pour affiner les r√©sultats
        try:
            self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
            self.use_reranking = True
            print("‚úÖ Reranker charg√© avec succ√®s")
        except:
            self.reranker = None
            self.use_reranking = False
            print("‚ö†Ô∏è Reranker non disponible - recherche simple uniquement")
        
        self.index = None
        self.chunks = []
        self.chunk_metadata = []
        self.index_file = os.path.join(config.FAISS_INDEX_DIR, "faiss_index.bin")
        self.chunks_file = os.path.join(config.FAISS_INDEX_DIR, "chunks.pkl")
        self.metadata_file = os.path.join(config.FAISS_INDEX_DIR, "metadata.pkl")
        
    async def initialize(self):
        """Initialise le service RAG"""
        await self._load_index()
        print(f"RAG Service initialis√©:")
        print(f"  - Index FAISS: {'Charg√©' if self.index else 'Vide'}")
        print(f"  - Nombre de chunks: {len(self.chunks)}")
        print(f"  - Reranking: {'Activ√©' if self.use_reranking else 'D√©sactiv√©'}")
        print(f"  - Seuil de similarit√©: {config.SIMILARITY_THRESHOLD}")
        
        # Debug: afficher quelques chunks
        if self.chunks:
            print(f"\nüìã Aper√ßu des chunks disponibles:")
            for i, chunk in enumerate(self.chunks[:3]):
                print(f"  {i+1}. {chunk[:100]}...")
    
    async def add_document_to_index(self, doc_id: str):
        """Ajoute un document √† l'index vectoriel"""
        print(f"Ajout du document {doc_id} √† l'index...")
        
        content = await document_service.get_document_content(doc_id)
        if not content:
            print(f"  ‚ùå Impossible de r√©cup√©rer le contenu du document {doc_id}")
            return
        
        print(f"  üìÑ Contenu r√©cup√©r√©: {len(content)} caract√®res")
        
        # D√©couper en chunks avec pr√©processing
        chunks = self._split_text_advanced(content)
        print(f"  ‚úÇÔ∏è Document d√©coup√© en {len(chunks)} chunks")
        
        if not chunks:
            print(f"  ‚ö†Ô∏è Aucun chunk g√©n√©r√© pour le document {doc_id}")
            return
        
        # Debug: afficher les premiers chunks
        print(f"  üìù Premiers chunks g√©n√©r√©s:")
        for i, chunk in enumerate(chunks[:2]):
            print(f"    {i+1}. {chunk[:150]}...")
        
        # G√©n√©rer les embeddings
        embeddings = self.embedder.encode(chunks)
        print(f"  üî¢ Embeddings g√©n√©r√©s: {embeddings.shape}")
        
        # Ajouter √† l'index
        if self.index is None:
            dimension = embeddings.shape[1]
            self.index = faiss.IndexFlatIP(dimension)
            print(f"  üÜï Nouvel index FAISS cr√©√© (dimension: {dimension})")
        
        # Normaliser les embeddings pour la similarit√© cosinus
        faiss.normalize_L2(embeddings)
        
        # Ajouter √† l'index
        self.index.add(embeddings.astype('float32'))
        
        # Ajouter aux chunks et m√©tadonn√©es
        start_idx = len(self.chunks)
        self.chunks.extend(chunks)
        for i, chunk in enumerate(chunks):
            self.chunk_metadata.append({
                'doc_id': doc_id,
                'chunk_idx': start_idx + i,
                'text': chunk
            })
        
        print(f"  ‚úÖ Document ajout√© √† l'index. Total chunks: {len(self.chunks)}")
        
        # Sauvegarder
        await self._save_index()
        print(f"  üíæ Index sauvegard√©")
    
    async def search_similar_chunks(self, query: str, k: int = 10) -> List[Dict]:
        """Recherche les chunks similaires avec reranking"""
        print(f"\nüîç Recherche avanc√©e pour: '{query}'")
        print(f"  - Index disponible: {'Oui' if self.index else 'Non'}")
        print(f"  - Nombre de chunks: {len(self.chunks)}")
        print(f"  - Reranking: {'Activ√©' if self.use_reranking else 'D√©sactiv√©'}")
        
        if self.index is None or len(self.chunks) == 0:
            print("  ‚ùå Pas d'index ou pas de chunks disponibles")
            return []
        
        # Pr√©processer la requ√™te
        processed_query = self._preprocess_query(query)
        print(f"  üîÑ Requ√™te pr√©process√©e: '{processed_query}'")
        
        # √âtape 1: Recherche vectorielle initiale avec plus de candidats
        initial_k = min(k * 3, len(self.chunks))  # 3x plus de candidats pour le reranking
        
        query_embedding = self.embedder.encode([processed_query])
        faiss.normalize_L2(query_embedding)
        
        scores, indices = self.index.search(query_embedding.astype('float32'), initial_k)
        
        print(f"  üéØ Recherche initiale effectu√©e, top {initial_k} candidats:")
        
        # Collecter tous les candidats avec leurs scores
        candidates = []
        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
            if idx >= 0 and idx < len(self.chunk_metadata):  # V√©rification de s√©curit√©
                metadata = self.chunk_metadata[idx]
                candidates.append({
                    'text': metadata['text'],
                    'doc_id': metadata['doc_id'],
                    'score': float(score),
                    'initial_rank': i + 1
                })
                print(f"    {i+1}. Score initial: {score:.4f}")
        
        if not candidates:
            print("  ‚ùå Aucun candidat trouv√©")
            return []
        
        # √âtape 2: Reranking si disponible
        if self.use_reranking and len(candidates) > 1:
            print(f"  üîÑ Reranking de {len(candidates)} candidats...")
            
            # Pr√©parer les paires (query, chunk) pour le reranker
            pairs = [(processed_query, candidate['text']) for candidate in candidates]
            
            # Calculer les scores de reranking
            rerank_scores = self.reranker.predict(pairs)
            
            # Mettre √† jour les scores et trier
            for i, candidate in enumerate(candidates):
                candidate['rerank_score'] = float(rerank_scores[i])
            
            # Trier par score de reranking
            candidates.sort(key=lambda x: x['rerank_score'], reverse=True)
            
            print(f"  üìä Scores apr√®s reranking:")
            for i, candidate in enumerate(candidates[:5]):
                print(f"    {i+1}. Rerank: {candidate['rerank_score']:.4f} | Initial: {candidate['score']:.4f}")
        
        # √âtape 3: Filtrage final et recherche par mots-cl√©s
        final_results = []
        
        # Ajout de la recherche par mots-cl√©s pour compl√©ter
        keyword_matches = self._keyword_search(query, candidates)
        
        # Combiner les r√©sultats
        seen_chunks = set()
        
        # D'abord les r√©sultats reranked/vectoriels
        for candidate in candidates:
            if len(final_results) >= k:
                break
                
            chunk_text = candidate['text']
            if chunk_text not in seen_chunks:
                # Crit√®res de s√©lection plus flexibles
                if (self.use_reranking and candidate.get('rerank_score', 0) > -5) or \
                   (not self.use_reranking and candidate['score'] >= config.SIMILARITY_THRESHOLD * 0.7):
                    
                    final_results.append(candidate)
                    seen_chunks.add(chunk_text)
        
        # Puis les matches par mots-cl√©s s'il reste de la place
        for match in keyword_matches:
            if len(final_results) >= k:
                break
            if match['text'] not in seen_chunks:
                final_results.append(match)
                seen_chunks.add(match['text'])
        
        print(f"  üìã {len(final_results)} chunks finaux retenus")
        
        # Debug: afficher les chunks retenus
        for i, result in enumerate(final_results):
            preview = result['text'][:100].replace('\n', ' ')
            print(f"    {i+1}. {preview}...")
        
        return final_results
    
    def _keyword_search(self, query: str, existing_candidates: List[Dict]) -> List[Dict]:
        """Recherche par mots-cl√©s pour compl√©ter la recherche vectorielle"""
        # Extraire les mots-cl√©s importants de la requ√™te
        query_lower = query.lower()
        keywords = []
        
        # Mots-cl√©s sp√©cifiques pour les conditions d'acc√®s
        if any(word in query_lower for word in ['condition', 'acc√®s', 'acces', 'admission', 'pr√©requis', 'prereq']):
            keywords.extend(['condition', 'acc√®s', 'acces', 'admission', 'pr√©requis', 'prereq', 'baccalaur√©at', 'bac'])
        
        # Mots-cl√©s pour ARI
        if any(word in query_lower for word in ['ari', 'administration', 'r√©seaux', 'reseaux', 'informatique']):
            keywords.extend(['ari', 'administration', 'r√©seaux', 'reseaux', 'informatique'])
        
        matches = []
        existing_texts = {c['text'] for c in existing_candidates}
        
        for i, chunk in enumerate(self.chunks):
            if chunk in existing_texts:
                continue
                
            chunk_lower = chunk.lower()
            score = 0
            
            # Compter les correspondances de mots-cl√©s
            for keyword in keywords:
                if keyword in chunk_lower:
                    score += 1
            
            if score > 0:
                matches.append({
                    'text': chunk,
                    'doc_id': self.chunk_metadata[i]['doc_id'] if i < len(self.chunk_metadata) else 'unknown',
                    'score': score * 0.1,  # Score artificiel pour le tri
                    'keyword_score': score,
                    'source': 'keyword_search'
                })
        
        # Trier par score de mots-cl√©s
        matches.sort(key=lambda x: x['keyword_score'], reverse=True)
        
        if matches:
            print(f"  üîë {len(matches)} correspondances par mots-cl√©s trouv√©es")
        
        return matches[:3]  # Limiter √† 3 r√©sultats par mots-cl√©s
    
    def _preprocess_query(self, query: str) -> str:
        """Pr√©processe la requ√™te pour am√©liorer la recherche"""
        # Normaliser les accents et caract√®res sp√©ciaux
        query = query.lower()
        
        # Corrections orthographiques courantes
        replacements = {
            'acce': 'acc√®s',
            'acces': 'acc√®s', 
            'prereq': 'pr√©requis',
            'reseaux': 'r√©seaux',
            'informatique': 'informatique'
        }
        
        for old, new in replacements.items():
            query = query.replace(old, new)
        
        return query
    
    async def has_relevant_context(self, query: str) -> bool:
        """V√©rifie s'il y a du contexte pertinent pour une requ√™te"""
        similar_chunks = await self.search_similar_chunks(query, k=1)
        has_context = len(similar_chunks) > 0
        print(f"  üéØ Contexte pertinent trouv√©: {'Oui' if has_context else 'Non'}")
        return has_context
    
    async def get_context_for_query(self, query: str) -> Tuple[str, List[str]]:
        """R√©cup√®re le contexte pertinent pour une requ√™te"""
        similar_chunks = await self.search_similar_chunks(query, k=5)
        
        if not similar_chunks:
            return "", []
        
        # Assembler le contexte en priorisant les meilleurs r√©sultats
        context_parts = []
        source_docs = set()
        
        for chunk in similar_chunks:
            # Nettoyer le texte du chunk
            clean_text = self._clean_chunk_text(chunk['text'])
            context_parts.append(clean_text)
            source_docs.add(chunk['doc_id'])
        
        context = "\n\n".join(context_parts)
        print(f"  üìù Contexte assembl√©: {len(context)} caract√®res, {len(source_docs)} documents sources")
        
        # Debug: afficher le contexte assembl√©
        print(f"  üìÑ CONTEXTE FINAL:")
        print(f"    {context[:300]}...")
        
        return context, list(source_docs)
    
    def _clean_chunk_text(self, text: str) -> str:
        """Nettoie le texte d'un chunk"""
        # Supprimer les caract√®res de contr√¥le et les caract√®res bizarres
        text = re.sub(r'[^\x20-\x7E\n\r\t\u00C0-\u017F]', '', text)
        
        # Normaliser les espaces
        text = re.sub(r'\s+', ' ', text)
        
        # Supprimer les espaces en d√©but et fin
        text = text.strip()
        
        return text
    
    def _split_text_advanced(self, text: str) -> List[str]:
        """D√©coupe le texte en chunks avec une logique avanc√©e"""
        if not text or not text.strip():
            return []
        
        # Nettoyer le texte d'abord
        text = self._clean_chunk_text(text)
        
        chunks = []
        
        # Essayer de d√©couper par sections logiques d'abord
        sections = re.split(r'\n\s*\n|(?=CONDITIONS D\'ACCES|OBJECTIF|D√âBOUCH√âS)', text)
        
        for section in sections:
            section = section.strip()
            if not section:
                continue
                
            # Si la section est petite, la garder enti√®re
            if len(section) <= config.CHUNK_SIZE:
                chunks.append(section)
            else:
                # Sinon, la d√©couper normalement
                chunks.extend(self._split_text_normal(section))
        
        return chunks
    
    def _split_text_normal(self, text: str) -> List[str]:
        """D√©coupage normal par taille"""
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + config.CHUNK_SIZE
            
            # Essayer de couper √† un espace ou un retour √† la ligne
            if end < len(text):
                for sep in ['\n\n', '\n', '. ', ' ']:
                    sep_pos = text.rfind(sep, start, end)
                    if sep_pos > start:
                        end = sep_pos + len(sep)
                        break
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end - config.CHUNK_OVERLAP
            if start >= end:
                start = end
        
        return chunks
    
    async def remove_document_from_index(self, doc_id: str):
        """Supprime un document de l'index (reconstruction compl√®te)"""
        print(f"Suppression du document {doc_id} de l'index...")
        
        # Filtrer les m√©tadonn√©es
        remaining_metadata = [m for m in self.chunk_metadata if m['doc_id'] != doc_id]
        remaining_chunks = [m['text'] for m in remaining_metadata]
        
        if not remaining_chunks:
            # R√©initialiser l'index si plus de chunks
            self.index = None
            self.chunks = []
            self.chunk_metadata = []
            print("  üóëÔ∏è Index r√©initialis√© (plus de documents)")
        else:
            # Reconstruire l'index
            embeddings = self.embedder.encode(remaining_chunks)
            faiss.normalize_L2(embeddings)
            
            dimension = embeddings.shape[1]
            self.index = faiss.IndexFlatIP(dimension)
            self.index.add(embeddings.astype('float32'))
            
            self.chunks = remaining_chunks
            self.chunk_metadata = remaining_metadata
            print(f"  üîÑ Index reconstruit: {len(remaining_chunks)} chunks restants")
        
        await self._save_index()
    
    async def _load_index(self):
        """Charge l'index depuis le disque"""
        try:
            if os.path.exists(self.index_file):
                self.index = faiss.read_index(self.index_file)
                print(f"Index FAISS charg√© depuis {self.index_file}")
            
            if os.path.exists(self.chunks_file):
                with open(self.chunks_file, 'rb') as f:
                    self.chunks = pickle.load(f)
                print(f"Chunks charg√©s: {len(self.chunks)}")
            
            if os.path.exists(self.metadata_file):
                with open(self.metadata_file, 'rb') as f:
                    self.chunk_metadata = pickle.load(f)
                print(f"M√©tadonn√©es charg√©es: {len(self.chunk_metadata)}")
                    
        except Exception as e:
            print(f"Erreur lors du chargement de l'index: {e}")
            self.index = None
            self.chunks = []
            self.chunk_metadata = []
    
    async def _save_index(self):
        """Sauvegarde l'index sur le disque"""
        try:
            if self.index is not None:
                faiss.write_index(self.index, self.index_file)
                print(f"Index FAISS sauvegard√©: {self.index_file}")
            
            with open(self.chunks_file, 'wb') as f:
                pickle.dump(self.chunks, f)
            
            with open(self.metadata_file, 'wb') as f:
                pickle.dump(self.chunk_metadata, f)
                
        except Exception as e:
            print(f"Erreur lors de la sauvegarde de l'index: {e}")

# Instance globale
rag_service = RAGService()